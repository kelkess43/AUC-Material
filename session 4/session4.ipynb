{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c12394",
   "metadata": {},
   "source": [
    "# üìò Lab Session 4: Building a Scorecard with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b71d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae30a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data = pd.read_csv('loans_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6d79f",
   "metadata": {},
   "source": [
    "## üß© Section 1 ‚Äì Dataset Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbf55d",
   "metadata": {},
   "source": [
    "### Dataset Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a851ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60505c71",
   "metadata": {},
   "source": [
    "### Dataset Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9dc24b",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f5e38",
   "metadata": {},
   "source": [
    "### ‚ùì Question: How can we identify and handle missing values in non-numerical (categorical) columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82aeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c08fb1",
   "metadata": {},
   "source": [
    "## General Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714125e",
   "metadata": {},
   "source": [
    "### Convert Emp length to Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\\+ years', '')\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', '0')\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n/a', '0')\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('+', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf53db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loan_data['emp_length_int'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1aca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fc9a3",
   "metadata": {},
   "source": [
    "### Convert Term to Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2260200",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['term_int'] = loan_data['term'].str.replace(' months', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['term_int']= pd.to_numeric(loan_data['term_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c326a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['annual_inc'].fillna(loan_data['annual_inc'].median(), inplace=True)\n",
    "loan_data['mths_since_earliest_cr_line'].fillna(0, inplace=True)\n",
    "loan_data['acc_now_delinq'].fillna(0, inplace=True)\n",
    "loan_data['total_acc'].fillna(0, inplace=True)\n",
    "loan_data['pub_rec'].fillna(0, inplace=True)\n",
    "loan_data['open_acc'].fillna(0, inplace=True)\n",
    "loan_data['inq_last_6mths'].fillna(0, inplace=True)\n",
    "loan_data['delinq_2yrs'].fillna(0, inplace=True)\n",
    "loan_data['emp_length_int'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07433aba",
   "metadata": {},
   "source": [
    "## üß© Section 2 ‚Äì Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6e1b4",
   "metadata": {},
   "source": [
    "### The distribution of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d76c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data[\"good_bad\"].value_counts().plot(kind=\"bar\")\n",
    "plt.xlabel(\"Target Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42faf13",
   "metadata": {},
   "source": [
    "### Box plots with respect to Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- Plot 1: mths_since_earliest_cr_line ---\n",
    "loan_data.boxplot(\n",
    "    column=[\"mths_since_earliest_cr_line\"],\n",
    "    by=\"good_bad\",\n",
    "    ax=axes[0],\n",
    "    grid=False,\n",
    "    patch_artist=True\n",
    ")\n",
    "axes[0].set_title(\"Months Since Earliest Credit Line by Good/Bad\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Good/Bad\")\n",
    "axes[0].set_ylabel(\"Months Since Earliest Credit Line\")\n",
    "\n",
    "# --- Plot 2: int_rate ---\n",
    "loan_data.boxplot(\n",
    "    column=[\"int_rate\"],\n",
    "    by=\"good_bad\",\n",
    "    ax=axes[1],\n",
    "    grid=False,\n",
    "    patch_artist=True\n",
    ")\n",
    "axes[1].set_title(\"Interest Rate by Good/Bad\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Good/Bad\")\n",
    "axes[1].set_ylabel(\"Interest Rate (%)\")\n",
    "\n",
    "# Improve spacing / cleanup\n",
    "plt.suptitle(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0aa80c",
   "metadata": {},
   "source": [
    "### Creating Dummy Variables (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':'),\n",
    "                     pd.get_dummies(loan_data['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n",
    "                     pd.get_dummies(loan_data['verification_status'], prefix = 'verification_status', prefix_sep = ':'),\n",
    "                     pd.get_dummies(loan_data['purpose'], prefix = 'purpose', prefix_sep = ':'),\n",
    "                     pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n",
    "                     pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]\n",
    "loan_data_dummies = pd.concat(loan_data_dummies, axis = 1)\n",
    "loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(\n",
    "    loan_data.drop('good_bad', axis=1),\n",
    "    loan_data['good_bad'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cae7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr = loan_data_inputs_test\n",
    "df_targets_prepr = loan_data_targets_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853274f5",
   "metadata": {},
   "source": [
    "## üß© Section 3 ‚Äì Weight of Evidence (WoE) & Information Value (IV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80014c07",
   "metadata": {},
   "source": [
    "### üîπ Fine Classing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['emp_length_int'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['int_rate_factor'] = pd.cut(df_inputs_prepr['int_rate'], 50)\n",
    "df_inputs_prepr['mths_since_earliest_cr_line_factor'] = pd.cut(df_inputs_prepr['mths_since_earliest_cr_line'], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46eb802",
   "metadata": {},
   "source": [
    "### üîπ Coarse Classing \n",
    "\n",
    "\n",
    "###     üîπCategories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118cdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WoE function for discrete unordered variables\n",
    "def woe_discrete(df, discrete_variabe_name, good_bad_variable_df):\n",
    "    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n",
    "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
    "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
    "    df = df.iloc[:, [0, 1, 3]]\n",
    "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
    "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
    "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
    "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
    "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
    "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
    "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
    "    df = df.sort_values(['WoE'])\n",
    "    df = df.reset_index(drop = True)\n",
    "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
    "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
    "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
    "    df['IV'] = df['IV'].sum()\n",
    "    return df\n",
    "\n",
    "def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n",
    "    x = np.array(df_WoE.iloc[:, 0].apply(str))\n",
    "    # Turns the values of the column with index 0 to strings, makes an array from these strings, and passes it to variable x.\n",
    "    y = df_WoE['WoE']\n",
    "    # Selects a column with label 'WoE' and passes it to variable y.\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    # Sets the graph size to width 18 x height 6.\n",
    "    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n",
    "    # Plots the datapoints with coordiantes variable x on the x-axis and variable y on the y-axis.\n",
    "    # Sets the marker for each datapoint to a circle, the style line between the points to dashed, and the color to black.\n",
    "    plt.xlabel(df_WoE.columns[0])\n",
    "    # Names the x-axis with the name of the column with index 0.\n",
    "    plt.ylabel('Weight of Evidence')\n",
    "    # Names the y-axis 'Weight of Evidence'.\n",
    "    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))\n",
    "    # Names the grapth 'Weight of Evidence by ' the name of the column with index 0.\n",
    "    plt.xticks(rotation = rotation_of_x_axis_labels)\n",
    "    # Rotates the labels of the x-axis a predefined number of degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdda28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = woe_discrete(df_inputs_prepr, 'grade', df_targets_prepr)\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_woe(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde37e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'home_ownership'\n",
    "df_temp = woe_discrete(df_inputs_prepr, 'home_ownership', df_targets_prepr)\n",
    "# We calculate weight of evidence.\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a2f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_woe(df_temp)\n",
    "# We plot the weight of evidence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:OTHER'],\n",
    "                                                      df_inputs_prepr['home_ownership:NONE'],df_inputs_prepr['home_ownership:ANY']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f243a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = woe_discrete(df_inputs_prepr, 'addr_state', df_targets_prepr)\n",
    "# We calculate weight of evidence.\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_woe(df_temp)\n",
    "# We plot the weight of evidence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ['addr_state:ND'] in df_inputs_prepr.columns.values:\n",
    "    pass\n",
    "else:\n",
    "    df_inputs_prepr['addr_state:ND'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['addr_state:ND_NE_IA_NV_FL_HI_AL'] = sum([df_inputs_prepr['addr_state:ND'], df_inputs_prepr['addr_state:NE'],\n",
    "                                              df_inputs_prepr['addr_state:IA'], df_inputs_prepr['addr_state:NV'],\n",
    "                                              df_inputs_prepr['addr_state:FL'], df_inputs_prepr['addr_state:HI'],\n",
    "                                                          df_inputs_prepr['addr_state:AL']])\n",
    "\n",
    "df_inputs_prepr['addr_state:NM_VA'] = sum([df_inputs_prepr['addr_state:NM'], df_inputs_prepr['addr_state:VA']])\n",
    "\n",
    "df_inputs_prepr['addr_state:OK_TN_MO_LA_MD_NC'] = sum([df_inputs_prepr['addr_state:OK'], df_inputs_prepr['addr_state:TN'],\n",
    "                                              df_inputs_prepr['addr_state:MO'], df_inputs_prepr['addr_state:LA'],\n",
    "                                              df_inputs_prepr['addr_state:MD'], df_inputs_prepr['addr_state:NC']])\n",
    "\n",
    "df_inputs_prepr['addr_state:UT_KY_AZ_NJ'] = sum([df_inputs_prepr['addr_state:UT'], df_inputs_prepr['addr_state:KY'],\n",
    "                                              df_inputs_prepr['addr_state:AZ'], df_inputs_prepr['addr_state:NJ']])\n",
    "\n",
    "df_inputs_prepr['addr_state:AR_MI_PA_OH_MN'] = sum([df_inputs_prepr['addr_state:AR'], df_inputs_prepr['addr_state:MI'],\n",
    "                                              df_inputs_prepr['addr_state:PA'], df_inputs_prepr['addr_state:OH'],\n",
    "                                              df_inputs_prepr['addr_state:MN']])\n",
    "\n",
    "df_inputs_prepr['addr_state:RI_MA_DE_SD_IN'] = sum([df_inputs_prepr['addr_state:RI'], df_inputs_prepr['addr_state:MA'],\n",
    "                                              df_inputs_prepr['addr_state:DE'], df_inputs_prepr['addr_state:SD'],\n",
    "                                              df_inputs_prepr['addr_state:IN']])\n",
    "\n",
    "df_inputs_prepr['addr_state:GA_WA_OR'] = sum([df_inputs_prepr['addr_state:GA'], df_inputs_prepr['addr_state:WA'],\n",
    "                                              df_inputs_prepr['addr_state:OR']])\n",
    "\n",
    "df_inputs_prepr['addr_state:WI_MT'] = sum([df_inputs_prepr['addr_state:WI'], df_inputs_prepr['addr_state:MT']])\n",
    "\n",
    "df_inputs_prepr['addr_state:IL_CT'] = sum([df_inputs_prepr['addr_state:IL'], df_inputs_prepr['addr_state:CT']])\n",
    "\n",
    "df_inputs_prepr['addr_state:KS_SC_CO_VT_AK_MS'] = sum([df_inputs_prepr['addr_state:KS'], df_inputs_prepr['addr_state:SC'],\n",
    "                                              df_inputs_prepr['addr_state:CO'], df_inputs_prepr['addr_state:VT'],\n",
    "                                              df_inputs_prepr['addr_state:AK'], df_inputs_prepr['addr_state:MS']])\n",
    "\n",
    "df_inputs_prepr['addr_state:WV_NH_WY_DC_ME_ID'] = sum([df_inputs_prepr['addr_state:WV'], df_inputs_prepr['addr_state:NH'],\n",
    "                                              df_inputs_prepr['addr_state:WY'], df_inputs_prepr['addr_state:DC'],\n",
    "                                              df_inputs_prepr['addr_state:ME'], df_inputs_prepr['addr_state:ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4596b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'verification_status'\n",
    "df_temp = woe_discrete(df_inputs_prepr, 'verification_status', df_targets_prepr)\n",
    "# We calculate weight of evidence.\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_woe(df_temp)\n",
    "# We plot the weight of evidence values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419dc84a",
   "metadata": {},
   "source": [
    "### ‚ùì Question: How to Do Coarse Classing for purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['purpose:educ__sm_b__wedd__ren_en__mov__house'] = sum([df_inputs_prepr['purpose:educational'], df_inputs_prepr['purpose:small_business'],\n",
    "                                                                 df_inputs_prepr['purpose:wedding'], df_inputs_prepr['purpose:renewable_energy'],\n",
    "                                                                 df_inputs_prepr['purpose:moving'], df_inputs_prepr['purpose:house']])\n",
    "df_inputs_prepr['purpose:oth__med__vacation'] = sum([df_inputs_prepr['purpose:other'], df_inputs_prepr['purpose:medical'],\n",
    "                                             df_inputs_prepr['purpose:vacation']])\n",
    "df_inputs_prepr['purpose:major_purch__car__home_impr'] = sum([df_inputs_prepr['purpose:major_purchase'], df_inputs_prepr['purpose:car'],\n",
    "                                                        df_inputs_prepr['purpose:home_improvement']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80259a2",
   "metadata": {},
   "source": [
    "### üîπ Continuous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WoE function for ordered discrete and continuous variables\n",
    "def woe_ordered_continuous(df, discrete_variabe_name, good_bad_variable_df):\n",
    "    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n",
    "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
    "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
    "    df = df.iloc[:, [0, 1, 3]]\n",
    "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
    "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
    "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
    "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
    "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
    "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
    "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
    "    #df = df.sort_values(['WoE'])\n",
    "    #df = df.reset_index(drop = True)\n",
    "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
    "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
    "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
    "    df['IV'] = df['IV'].sum()\n",
    "    return df\n",
    "# Here we define a function similar to the one above, ...\n",
    "# ... with one slight difference: we order the results by the values of a different column.\n",
    "# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33489053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = woe_ordered_continuous(df_inputs_prepr, 'term_int', df_targets_prepr)\n",
    "# We calculate weight of evidence.\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_woe(df_temp, 90)\n",
    "# We plot the weight of evidence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca662d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['term:36'] = np.where((df_inputs_prepr['term_int'] == 36), 1, 0)\n",
    "df_inputs_prepr['term:60'] = np.where((df_inputs_prepr['term_int'] == 60), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = woe_ordered_continuous(df_inputs_prepr, 'int_rate_factor', df_targets_prepr)\n",
    "# We calculate weight of evidence.\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62665b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_woe(df_temp, 90)\n",
    "# We plot the weight of evidence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['int_rate:<9.548'] = np.where((df_inputs_prepr['int_rate'] <= 9.548), 1, 0)\n",
    "df_inputs_prepr['int_rate:9.548-12.025'] = np.where((df_inputs_prepr['int_rate'] > 9.548) & (df_inputs_prepr['int_rate'] <= 12.025), 1, 0)\n",
    "df_inputs_prepr['int_rate:12.025-15.74'] = np.where((df_inputs_prepr['int_rate'] > 12.025) & (df_inputs_prepr['int_rate'] <= 15.74), 1, 0)\n",
    "df_inputs_prepr['int_rate:15.74-20.281'] = np.where((df_inputs_prepr['int_rate'] > 15.74) & (df_inputs_prepr['int_rate'] <= 20.281), 1, 0)\n",
    "df_inputs_prepr['int_rate:>20.281'] = np.where((df_inputs_prepr['int_rate'] > 20.281), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
    "df_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_earliest_cr_line_factor', df_targets_prepr)\n",
    "# We calculate weight of evidence.\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_woe(df_temp, 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_prepr['mths_since_earliest_cr_line:<140'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140)), 1, 0)\n",
    "df_inputs_prepr['mths_since_earliest_cr_line:141-164'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140, 165)), 1, 0)\n",
    "df_inputs_prepr['mths_since_earliest_cr_line:165-247'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(165, 248)), 1, 0)\n",
    "df_inputs_prepr['mths_since_earliest_cr_line:248-270'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(248, 271)), 1, 0)\n",
    "df_inputs_prepr['mths_since_earliest_cr_line:271-352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(271, 353)), 1, 0)\n",
    "df_inputs_prepr['mths_since_earliest_cr_line:>352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(353, int(df_inputs_prepr['mths_since_earliest_cr_line'].max()))), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9868e34a",
   "metadata": {},
   "source": [
    "### ‚ùì Question: How to Do Coarse Classing for emp_length_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01d45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b558068b",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae7645",
   "metadata": {},
   "source": [
    "## Section 4 ‚Äì Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ed850",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = [\n",
    " 'home_ownership:MORTGAGE',\n",
    " 'home_ownership:OWN',\n",
    " 'verification_status:Not Verified',\n",
    " 'verification_status:Source Verified',\n",
    " 'verification_status:Verified',\n",
    " 'purpose:credit_card',\n",
    " 'purpose:debt_consolidation',\n",
    " 'addr_state:CA',\n",
    " 'addr_state:NY',\n",
    " 'term:36',\n",
    " 'term:60',\n",
    " 'home_ownership:RENT_OTHER_NONE_ANY',\n",
    " 'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    " 'addr_state:NM_VA',\n",
    " 'addr_state:OK_TN_MO_LA_MD_NC',\n",
    " 'addr_state:UT_KY_AZ_NJ',\n",
    " 'addr_state:AR_MI_PA_OH_MN',\n",
    " 'addr_state:RI_MA_DE_SD_IN',\n",
    " 'addr_state:GA_WA_OR',\n",
    " 'addr_state:WI_MT',\n",
    " 'addr_state:IL_CT',\n",
    " 'addr_state:KS_SC_CO_VT_AK_MS',\n",
    " 'addr_state:WV_NH_WY_DC_ME_ID',\n",
    " 'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    " 'purpose:oth__med__vacation',\n",
    " 'purpose:major_purch__car__home_impr',\n",
    " 'int_rate:<9.548',\n",
    " 'int_rate:9.548-12.025',\n",
    " 'int_rate:12.025-15.74',\n",
    " 'int_rate:15.74-20.281',\n",
    " 'int_rate:>20.281',\n",
    " 'mths_since_earliest_cr_line:<140',\n",
    " 'mths_since_earliest_cr_line:141-164',\n",
    " 'mths_since_earliest_cr_line:165-247',\n",
    " 'mths_since_earliest_cr_line:248-270',\n",
    " 'mths_since_earliest_cr_line:271-352',\n",
    " 'mths_since_earliest_cr_line:>352',\n",
    " 'emp_length:0',\n",
    " 'emp_length:1',\n",
    " 'emp_length:2-4',\n",
    " 'emp_length:5-6',\n",
    " 'emp_length:7-9',\n",
    " 'emp_length:10'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_inputs =df_inputs_prepr[dummy_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_categories = [\n",
    " 'home_ownership:RENT_OTHER_NONE_ANY',\n",
    " 'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    " 'verification_status:Verified',\n",
    " 'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    " 'term:60',\n",
    " 'emp_length:0',\n",
    " 'int_rate:>20.281',\n",
    " 'mths_since_earliest_cr_line:<140'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d99ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_inputs = logistic_regression_inputs.drop(ref_categories,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add14a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()\n",
    "reg.fit(logistic_regression_inputs, df_targets_prepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = logistic_regression_inputs.columns.values\n",
    "feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "summary_table['Coefficients'] = np.transpose(reg.coef_)\n",
    "summary_table.index = summary_table.index + 1\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "summary_table = summary_table.sort_index()\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])\n",
    "# We create a new dataframe with one column. Its values are the values from the 'reference_categories' list.\n",
    "# We name it 'Feature name'.\n",
    "df_ref_categories['Coefficients'] = 0\n",
    "# We create a second column, called 'Coefficients', which contains only 0 values.# We create a third column, called 'p_values', with contains only NaN values.\n",
    "df_ref_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17180fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard = pd.concat([summary_table, df_ref_categories])\n",
    "# Concatenates two dataframes.\n",
    "df_scorecard = df_scorecard.reset_index()\n",
    "# We reset the index of a dataframe.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbf149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]\n",
    "# We create a new column, called 'Original feature name', which contains the value of the 'Feature name' column,\n",
    "# up to the column symbol.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 300\n",
    "max_score = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()\n",
    "# Up to the 'min()' method everything is the same as in te line above.\n",
    "# Then, we aggregate further and sum all the minimum values.\n",
    "min_sum_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()\n",
    "# Up to the 'min()' method everything is the same as in te line above.\n",
    "# Then, we aggregate further and sum all the maximum values.\n",
    "max_sum_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)\n",
    "# We multiply the value of the 'Coefficients' column by the ration of the differences between\n",
    "# maximum score and minimum score and maximum sum of coefficients and minimum sum of cefficients.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bfd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Calculation'][0] = ((df_scorecard['Coefficients'][0] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score\n",
    "# We divide the difference of the value of the 'Coefficients' column and the minimum sum of coefficients by\n",
    "# the difference of the maximum sum of coefficients and the minimum sum of coefficients.\n",
    "# Then, we multiply that by the difference between the maximum score and the minimum score.\n",
    "# Then, we add minimum score. \n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()\n",
    "# We round the values of the 'Score - Calculation' column.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n",
    "# Sums all minimum values.\n",
    "min_sum_score_prel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n",
    "# Sums all maximum values.\n",
    "max_sum_score_prel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
